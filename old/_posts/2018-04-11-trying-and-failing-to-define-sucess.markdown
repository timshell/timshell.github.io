---
layout: post
title: Trying (and Failing) to Define Success
description: 
category: articles
blog: true
date: 2018-04-11
tags: []
---

Whether it's academics, athletics, or something else, I want to be good at whatever I set my mind to. As I am about to embark on a career of science, I have increasingly thought about what does being good even mean. Is it well-defined? Is it inherently subjective? If so, how does that affect my opportunity to get better?

Thinking about the past, I have been drawn to objective measures of success. For example, I have [competitively run](/trial-of-miles-miles-of-trials) for seven years now. The goal there is extremely simple: minimize the time it takes to run a certain distance. Once a race is done, I have a pretty straightforward evaluation of how I am, both in relation to my past self as well as other people.

As a computer scientist, I implicitly think about being good using reinforcement learning. Reinforcement learning is a subfield of machine learning where agents act in an environment to try to maximize some type of reward function. It became popular in the public when Google DeepMind's [AlphaGo defeated Go world champion Lee Sodol](https://www.nature.com/articles/nature16961). Reinforcement learning was the workhorse behind DeepMind's success. Here, the reward function was whether it won or lost and the machine had to explore strategies in order to maximize the probability it won.

Relating this back to the running scenario, my reward function is the time it takes to run a race. The faster I am, the higher my reward. Because I know this reward function, I can do research to tailor my strategy. That is, to be successful, I just need to get faster and there are many ways to do so.[^1]

In middle school, I wanted to be good at math. In that case, however, I assumed the wrong reward function: math competitions. While I was decently good at them, I [struggled in the classroom](/rethinking-secondary-mathematics-education). The reason being I spent my time memorizing shortcuts and tricks to solve contrived problems rather than gain a deep understanding of the mathematical concepts. So, while my goal was to get better at math, the fact that I assumed the wrong reward function and optimized for that meant I didn't reach the level I wanted to. Even worse, I deluded myself into thinking I was good because how I fared in competitions.

In the present-day, I have a much better understanding of what it means to be good at math. For example, I know proofs and abstraction are much more important than memorization of multiplication tricks. But, I also don't have a measure for how good I am at proofs. I want to be able to use the reward function to really understand my strengths and weaknesses so I can create a better strategy. While I can get better at math by contributing X hours a day towards it, I won't be getting better optimally.

The same uncertainty is happening with my future at a scientist. I need to have lots of knowledge of the field as well as the ability to design experiments, develop original theories, communicate with audiences, etc. But, there is a lack of an objective function.

Certain objective measures have been proposed to measure how good of a scientist someone is. In academia, many of these revolve around citation and publication counts. Yet, now we get into the problem of Goodhart's Law, which is paraphrased as "When a measure becomes a target, it ceases to be a good measure." Citation or publication counts can be a good heuristic when someone is not trying to optimize those numbers. But science for the pursuit of maximizing those measures has lead to the ongoing replication crisis. Scientists publish results of effects that are not actually there by employing questionable scientific practices to increase the probability of getting a statistically significant result. We use statistically significant hypotheses as a proxy for significant scientific effects, and that has led to [most published researching findings being false](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124). Science has tried to optimize a reward function that slightly deviates from the actual purpose of itself, and the subject is now struggling to resolve the consequences.

Consider the great scientists: Galileo, Darwin, Newton, Einstein. Imagine if they were worried about Google Scholar citation counts. They took the time to develop highly original and predictive theories that accurately explain reality today. That is science. Yet, many scientists rightfully ask whether someone like Darwin would get a job in academia today. Scientific institutions have defined the reward function in science to be a function of citations and publications, and perhaps the overemphasis of that reward function means science like the greats did won't be carried out. Einstein is certainly a highly-cited author, but he is not a top twenty cited scholar in all of physics. It is useful citation count as a heuristic for his impact because he (presumably) was not trying to maximize it. But, as Goodhart's Law states, once citation counts becomes a measure of how good a scientist one is, it ceases to be a good measure because then people just try to game the system. This is not to say that citation counts are useless. Rather, it should interpreted in a much more qualified manner.

A seemingly unrelated way to think of scientific success is the argument from Thomas Kuhn's *Structure of Scientific Revolutions*. Kuhn is the philosopher of science that popularized the phrase 'paradigm shift'. He separates science into two distinctions: normal science and revolutionary science. Normal science operates under a paradigm, which is when there is an overarching theory that has its own definitions of concepts. The theory makes predictions and scientists are tasked with creating experiments that prove or disprove the theory's hypotheses.

On the other hand, revolutionary science is when paradigms are in a transitory period. New paradigms offer new theories as well as redefine terminology (e.g. gravity being a force in Newtonian physics but now space-time curvature in Einstein's paradigm). It seems from Kuhn's distinction that the great scientists are the ones who created new paradigms and brought about revolutionary science.

Is the goal of a scientist to be successful in normal science or bring about a paradigm shift? And, do those two goals have similar or different strategies? The strategy for success in normal science is relatively straightforward: under the paradigm, generate hypotheses and conduct experiments confirming or dis-confirming the overarching theory. Is doing this how scientists get disillusioned with a current paradigm and create a new one? That is, do the best 'normal scientists' become the 'revolutionary scientists'?

One reason to think this is not true is that oftentimes there is not sufficient experimental evidence to support new revolutionary paradigms in the early stages. Rather, it is held as a dogmatic belief by a few individuals. While history of science taught in primary and secondary school implies that new theories were always better predictors than old theories, such is not usually the case. It took more than half a century after Copernicus's death until the experimental evidence of his heliocentric theory matched that of Ptolemy's theory.

Look at the paradigm shift in artificial intelligence today when deep learning took over. Neural nets were invented more than half a century ago, but went became out of fashion for decades. It was the [stubbornness of Geoff Hinton](https://torontolife.com/tech/ai-superstars-google-facebook-apple-studied-guy/) (and others) that led to the advent of the current state of machine learning we see today. Today, we praise Hinton for sticking to his beliefs in the face of pressure to abandon his research. We praise Copernicus for championing the heliocentric theory. And, we praise Lavoisier for replacing phlogiston. Yet, all these scientists had these beliefs when it was seemingly irrational to have them. Thus, there seems to be a degree of irrationality, stubbornness, and close-mindedness it takes to be a great scientist. But before we champion these ideals in scientists, how many scientists had these traits, thought they would be great scientists, and then be proven wrong? 

Perhaps the path for success for revolutionary scientists and normal scientists are in fact different. And, perhaps one is inherently riskier than the other. That risk may lead to a small chance that your name will be in textbooks for generations to come or the (much) larger chance that your theories will be dismissed as hearsay.

We can also turn to philosophy and examine how the greats fared there. The great philosophers did not have bulletproof logic. Otherwise, their arguments would be for the most part accepted today. Descartes *cogito ergo sum* is influential, but his justification is generally considered inadequate. There are very few dualists in philosophy of mind today. By contemporary standards, Kant's arguments would be awful. But, he is considered a great because he changed the way we did philosophy. He self-righteously analogized his contribution to philosophy as a Copernican revolution.

In philosophy, like science, we are taught that being incremental is good. Yet, being incremental is not how you influence your field. That is done by being incredibly original. So again we have the question of whether being incremental for some time is necessary for being revolutionary? If not, why do we teach philosophers and scientists to be incremental? Some may say yes, it is only after grappling with concepts for years that we are truly able to understand the flaws and generate new ideas and theories. Others might take a more rationalist approach: we are not going to pay attention to the person who generates all these crazy ideas even if it is probable that one of them (unbeknown to us, though) is influential. Rather, we listen to the person who is trained in the same manner we are but eventually strays off the path and stakes their reputation on their new ideas.

We don't have precise characterizations of what it means to be a good scientist, philosopher, etc. But, we have institutions in place that try to capture this: universities, journals, etc. Because of Goodhart's Law, it is probably good we don't have an objective function that fully defines an individual's contribution to the field. For, that would lead to even more over-optimization and deviation from what we intuitively consider to be true success.

It is frustrating that the path is not as well-defined as I may like it to be. But again, this is why the fields are not completed. As we make progress in each field, we also probably change our conception of what success looks like. That being said, it is important to keep on having the conversation about what are current standards so that we get better in how we evaluate and make sure we eradicate biases in the system.

---

[^1] I should note that this need not be every runner's goal. Something I often see at the collegiate level is that people want to win championships. While this is seemingly similar to my goal, there are slight differences. For example, championship races are often slow because the runners jog for a long time and then have a sprint finish. To be successful in that endeavor requires a lot more sprint speed than needed in my goal. Of course one must be very fast in order to just get to the championship level and there is an overlap between runners who set records and runners who win championships, but I have selectively focused on one.